<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Ceph后端存储引擎BlueStore</title>
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/epub.css" type="text/css" />
    <link rel="stylesheet" href="../_static/theme_overrides.css" type="text/css" /> 
  </head><body>

    <div class="document">
      <div class="documentwrapper">
          <div class="body" role="main">
            
  <section id="cephbluestore">
<span id="bluestore"></span><h1>Ceph后端存储引擎BlueStore</h1>
<p>从Ceph Luminous v12.2.z 开始，默认采用了新型的BlueStore作为Ceph OSD后端，用于管理存储磁盘。</p>
<p>BlueStore的优势：</p>
<ul class="simple">
<li><p>对于大型写入操作，避免了原先FileStore的两次写入 （注意，很多FileStore将journal日志存放到独立到SSD上，也能够获得类似的性能提升，不过分离journal的部署方式是绕开FileStore的短板）</p></li>
<li><p>对于小型随机写入，BlueStore比 FileStore with journal 性能还要好</p></li>
<li><p>对于Key/value数据BlueStore性能明显提升</p></li>
<li><p>当集群数据接近爆满时，BlueStore避免了FileStore的性能陡降问题</p></li>
<li><p>在BlueStore上使用raw库的小型顺序读性能有降低，和BlueStore不采用预读(readahead)有关，但可以通过上层接口（如RBD和CephFS）来实现性能提升</p></li>
<li><p>BlueStore在RBD卷或CephFS文件中采用了copy-on-write提升了性能</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>当前在Red Hat Ceph 3.x中， <a class="reference external" href="https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/3/html-single/administration_guide/index#osd-bluestore">Red Hat Ceph BlueStore</a><span class="link-target"> [https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/3/html-single/administration_guide/index#osd-bluestore]</span> 后端是作为技术预览提供。</p>
<p>micron公司 <a class="reference external" href="https://www.micron.com/about/blog/2018/may/ceph-bluestore-vs-filestoreblock-performance-comparison-when-leveraging-micron-nvme-ssds">Ceph BlueStore vs. FileStore: Block performance comparison when leveraging Micron NVMe SSDs.</a><span class="link-target"> [https://www.micron.com/about/blog/2018/may/ceph-bluestore-vs-filestoreblock-performance-comparison-when-leveraging-micron-nvme-ssds]</span> 评测概况:</p>
<ul class="simple">
<li><p>4KB随机写IOPS性能增强 18% ，平均延迟降低 15%，以及 99.99% 长尾延迟最大降低 80%</p></li>
<li><p>4KB随机读性能根据BlueStore配置的更高队列得到更好的性能</p></li>
</ul>
<p>micron公司提供了 <a class="reference external" href="https://www.micron.com/-/media/client/global/documents/products/other-documents/micron_9200_ceph_3,-d-,0_reference_architecture.pdf?la=en">Micron Accelerated Ceph Storage Solutions</a><span class="link-target"> [https://www.micron.com/-/media/client/global/documents/products/other-documents/micron_9200_ceph_3,-d-,0_reference_architecture.pdf?la=en]</span> 可以作为Ceph硬件架构部署的参考。</p>
</div>
<section id="id1">
<h2>BlueStore工作原理</h2>
<p>Ceph OSD执行两个功能：在网络中和其他OSD之间复制数据（多副本），以及在本地存储设备上存储数据。</p>
<p>传统的Ceph OSD是将数据存储到现有的文件存储模块，例如XFS文件系统。但是这样的性能开销较大，因为需要实现Ceph数据到POSIX的转换。</p>
<img alt="../_images/filestore-vs-bluestore-2.png.webp" src="../_images/filestore-vs-bluestore-2.png.webp" />
<p>BlueStore是在底层裸块设备上建立的存储系统，内建了RocksDB key/value 数据库用于管理内部元数据。一个小型的内部接口组件称为 <code class="docutils literal notranslate"><span class="pre">BludFS</span></code> 实现了类似文件系统的接口，以便提供足够功能让RocksDB来存储它的”文件”并向BlueStroe共享相同的裸设备。</p>
<p>和之前的FileStore不同的是，BlueStore类似分区和挂载点，例如在一个FileStore OSD，你会看到类似:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ lsblk
…
sdb      8:16   0 931.5G  0 disk
├─sdb1   8:17   0 930.5G  0 part /var/lib/ceph/osd/ceph-56
└─sdb2   8:18   0  1023M  0 part
…
$ df -h
…
/dev/sdb1       931G  487G  444G  53% /var/lib/ceph/osd/ceph-56
$ ls -al /var/lib/ceph/osd/ceph-56
…
drwxr-xr-x 180 root root 16384 Aug 30 21:55 current
lrwxrwxrwx   1 root root    58 Jun  4  2015 journal -&gt; /dev/disk/by-partuuid/538da076-0136-4c78-9af4-79bb40d7cbbd
</pre></div>
</div>
<p>可以看到上述FileStore有一个很小的日志分区（journal partition），通常这个日志分区位于一个独立的SSD上，一个日志系统链接（journal symlink）从数据目录指向这个独立的日志分区，并且一个当前目录包含了所有实际对象文件。</p>
<p>对比之下，在BlueStore OSD，你会看到:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ lsblk
…
sdf      8:80   0   3.7T  0 disk
├─sdf1   8:81   0   100M  0 part /var/lib/ceph/osd/ceph-75
└─sdf2   8:82   0   3.7T  0 part
…
$ df -h
…
/dev/sdf1        97M  5.4M   92M   6% /var/lib/ceph/osd/ceph-75
…
$ ls -al /var/lib/ceph/osd/ceph-75
…
lrwxrwxrwx 1 ceph ceph   58 Aug  8 18:33 block -&gt; /dev/disk/by-partuuid/80d28eb7-a7e7-4931-866d-303693f1efc4
</pre></div>
</div>
<p>在BlueStore中目录数据目录技术一个微小分区（100MB），而系统链接到一个块设备上，也就是BlueStore存储数据的块设备，所以IO是从ceph-osd进程直接访问raw设备的（使用Linux异步libaio结构）。</p>
<p>虽然不能和以前一样直接查看BlueStore中的对象文件，但是如果OSD停止，依然可以通过FUSE <code class="docutils literal notranslate"><span class="pre">mount</span></code> OSD数据:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mkdir</span> <span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="n">foo</span>
<span class="n">ceph</span><span class="o">-</span><span class="n">objectstore</span><span class="o">-</span><span class="n">tool</span> <span class="o">--</span><span class="n">op</span> <span class="n">fuse</span> <span class="o">--</span><span class="n">data</span><span class="o">-</span><span class="n">path</span> <span class="o">/</span><span class="n">var</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">ceph</span><span class="o">/</span><span class="n">osd</span><span class="o">/</span><span class="n">ceph</span><span class="o">-</span><span class="mi">75</span> <span class="o">--</span><span class="n">mountpoint</span> <span class="o">/</span><span class="n">mnt</span><span class="o">/</span><span class="n">foo</span>
</pre></div>
</div>
</section>
<section id="id2">
<h2>多设备</h2>
<p>BuleStore可以组合慢速和快速设备，类似FileStore，并通常能够更多使用快速设备。在FileStore中，日志设备只用于写，通常位于较快多SSD。在BlueStore中，内建的日志需要保持稳定，更为轻量级，类似元数据日志只记录小型写入。</p>
<p>BlueStore可以管理最多3个设备：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">main</span></code> 设备（块系统链接）存储目标队形以及元数据</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">db</span></code> 设备是可选的，存储metadata(RocksDB)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">WAL</span></code> 设备可选只用于内部日志（RocksDB预写日志）</p></li>
</ul>
<p>通常建议使用尽可能多的SSD空间给OSD使用，并且将SSD用于 <code class="docutils literal notranslate"><span class="pre">block.db</span></code> 设备。当使用 <code class="docutils literal notranslate"><span class="pre">ceph-disk</span></code> 结合 <code class="docutils literal notranslate"><span class="pre">--block.db</span></code> 参数:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span><span class="o">-</span><span class="n">disk</span> <span class="n">prepare</span> <span class="o">/</span><span class="n">dev</span><span class="o">/</span><span class="n">sdb</span> <span class="o">--</span><span class="n">block</span><span class="o">.</span><span class="n">db</span> <span class="o">/</span><span class="n">dev</span><span class="o">/</span><span class="n">sdc</span>
</pre></div>
</div>
<p>如果没有采用独立的 <code class="docutils literal notranslate"><span class="pre">block.db</span></code> 设备，则默认存储在主设备，占用1%空间。这个预先配置空间可以通过 <code class="docutils literal notranslate"><span class="pre">bluestore_block_db_size</span></code> 配置选项修改。如果使用3个设备（节约成本）：主设备使用HDD，db设备使用SSD，最小的NVDIMM设备用于WAL。</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>详细的BlueStore配置信息可以参考 <a class="reference external" href="http://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#devices">BlueStore configuration guide</a><span class="link-target"> [http://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#devices]</span></p>
</div>
</section>
<section id="id3">
<h2>内存使用</h2>
<p>由于BlueStore是作为OSD的用户空间实现，所以自己管理缓存，通过一些内存管理工具实现。</p>
<p>BlueStore的底线是 <code class="docutils literal notranslate"><span class="pre">bluestore_cache_size</span></code> 配置选贤，控制每个OSD所使用的BlueStore缓存消耗的内存。默认是每个HDD后端OSD使用1GB内存，而SSD后端OSD则使用3GB内存。不过你可以根据自己环境来调整合适值。</p>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>内存记账当前是不完善的：使用了tcmalloc，会导致有时候实际分配超过设置值。随着时间推移堆栈会碎片化，而内存碎片化使得部分释放内存不能返回给操作系统。结果就是，通常会出现BlueStore(和OSS)认为使用的内核和进程实际使用内存(RSS)差异有时候出现1.5倍差异。对比 <code class="docutils literal notranslate"><span class="pre">ceph-osd</span></code> 的进程RSS消耗内存和通过 <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">daemon</span> <span class="pre">osd.&lt;id&gt;</span> <span class="pre">dump_mempools</span></code> 输出的值就可以看到这个差异。目前ceph项目还在解决这个问题。（2017年9月）</p>
</div>
</section>
<section id="id4">
<h2>校验</h2>
<p>BlueStore计算、存储和验证校验所有存储的数据和元数据。任何时候从磁盘读取数据，数据在输出给系统其他部分（或用户）使用之前都会使用校验来验证数据正确性。默认使用 <code class="docutils literal notranslate"><span class="pre">crc32c</span></code> checksum，也提供其他校验选项（ <code class="docutils literal notranslate"><span class="pre">xxhash32</span></code> ， <code class="docutils literal notranslate"><span class="pre">xxhash64</span></code> ），甚至支持使用降级的 <code class="docutils literal notranslate"><span class="pre">crc32c</span></code> （例如，使用标准校验32位数据中的8位或者16位）来降低元数据跟踪（牺牲了数据可靠性）。也支持完全关闭数据校验（但不推荐此方式）。详细可参考 <a class="reference external" href="http://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#checksums">文档的checksum部分</a><span class="link-target"> [http://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#checksums]</span> 。</p>
</section>
<section id="id5">
<h2>压缩</h2>
<p>BlueStore可以使用zlib, snappy 或者 lz4 实现透明压缩。默认BlueStore关闭了压缩功能，但是可以全局激活或者针对特定存储池激活，也提供了当RADOS客户端访问数据时选择性激活。详情请参考 <a class="reference external" href="http://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#inline-compression">文档的压缩章节</a><span class="link-target"> [http://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#inline-compression]</span> 。</p>
</section>
<section id="id7">
<h2>转换现有集群到BlueStore</h2>
<p>在一个单一集群中每个OSD可以同时包含一些FileStore OSD和一些BlueStore OSD。一个升级的集群可以持续完成转换，即新OSD采用BlueStore来不断轮转替换旧的FileStore，通过不断下线旧的FileStore并添加新的FileStore（默认启用BlueStore）利用数据健康恢复将多副本数据复制回来。这个过程可以平滑安全实现，详情请参考 <a class="reference external" href="http://docs.ceph.com/docs/master/rados/operations/bluestore-migration/">BlueStore migration</a><span class="link-target"> [http://docs.ceph.com/docs/master/rados/operations/bluestore-migration/]</span> 。</p>
</section>
<section id="id8">
<h2>参考</h2>
<ul class="simple">
<li><p><a class="reference external" href="https://ceph.com/community/new-luminous-bluestore/">New in Luminous: BlueStore</a><span class="link-target"> [https://ceph.com/community/new-luminous-bluestore/]</span></p></li>
<li><p><a class="reference external" href="https://www.micron.com/about/blog/2018/may/ceph-bluestore-vs-filestoreblock-performance-comparison-when-leveraging-micron-nvme-ssds">Ceph BlueStore vs. FileStore: Block performance comparison when leveraging Micron NVMe SSDs.</a><span class="link-target"> [https://www.micron.com/about/blog/2018/may/ceph-bluestore-vs-filestoreblock-performance-comparison-when-leveraging-micron-nvme-ssds]</span></p></li>
<li><p><a class="reference external" href="https://blog.rackspace.com/maximize-performance-ceph-storage-solution">Maximize the Performance of Your Ceph Storage Solution</a><span class="link-target"> [https://blog.rackspace.com/maximize-performance-ceph-storage-solution]</span> - Racespace的私有云客户案例，采用NVMe存储协议和持久化内存加速的BlueStore存储后端</p></li>
</ul>
</section>
</section>


            <div class="clearer"></div>
          </div>
      </div>
      <div class="clearer"></div>
    </div>
  </body>
</html>